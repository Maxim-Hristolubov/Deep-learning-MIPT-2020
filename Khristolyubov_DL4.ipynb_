{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"},"colab":{"name":"Khristolyubov_DL4.ipynb\"","provenance":[{"file_id":"https://github.com/nadiinchi/dl_labs/blob/master/ht_attention.ipynb","timestamp":1605726101805}],"toc_visible":true},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"tVKXeDhTu8y0"},"source":["<h1 align=\"center\"> Image Captioning With Attention</h1>\n","\n","In this notebook you will teach network to use attention during captioning images.\n","\n","Here is what we need to do:\n","1. Take pretrained VGG19 to build feature vectors for positions of images.\n","2. Stack LSTM with attention on top of that.\n","3. Train the model, draw attention maps.\n","\n","This assignment is based on Alexander Panin's captioning assignment (https://github.com/yandexdataschool/Practical_DL/blob/spring2019/homework04/homework04_basic_part2_image_captioning.ipynb) and on the paper \"Show, Attend and Tell\" (https://arxiv.org/abs/1502.03044)."]},{"cell_type":"code","metadata":{"id":"4FkFQEr9u8y0","executionInfo":{"status":"ok","timestamp":1606225966943,"user_tz":-180,"elapsed":899,"user":{"displayName":"Максим Евгеньевич Христолюбов","photoUrl":"","userId":"07722095761291801978"}}},"source":["# import os\n","# os.environ['CUDA_VISIBLE_DEVICES'] = '0'"],"execution_count":1,"outputs":[]},{"cell_type":"code","metadata":{"id":"_HGgzQcqu8y0","executionInfo":{"status":"ok","timestamp":1606225967706,"user_tz":-180,"elapsed":1654,"user":{"displayName":"Максим Евгеньевич Христолюбов","photoUrl":"","userId":"07722095761291801978"}}},"source":["import json"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"jESz1Q4au8y1","executionInfo":{"status":"ok","timestamp":1606225967707,"user_tz":-180,"elapsed":1650,"user":{"displayName":"Максим Евгеньевич Христолюбов","photoUrl":"","userId":"07722095761291801978"}}},"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","%matplotlib inline"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"XMw3Xe2Pu8y1","executionInfo":{"status":"ok","timestamp":1606225967707,"user_tz":-180,"elapsed":1646,"user":{"displayName":"Максим Евгеньевич Христолюбов","photoUrl":"","userId":"07722095761291801978"}}},"source":["from tqdm import tqdm\n","import h5py"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"eb5jjhuOu8y1","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1606225972432,"user_tz":-180,"elapsed":6349,"user":{"displayName":"Максим Евгеньевич Христолюбов","photoUrl":"","userId":"07722095761291801978"}},"outputId":"7cf36582-082a-48fa-a504-b3604e6333c7"},"source":["# you may need to install h5py, tqdm\n","!pip install h5py tqdm\n","import sys\n","!{sys.executable} -m pip install h5py tqdm"],"execution_count":5,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (2.10.0)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (4.41.1)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from h5py) (1.15.0)\n","Requirement already satisfied: numpy>=1.7 in /usr/local/lib/python3.6/dist-packages (from h5py) (1.18.5)\n","Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (2.10.0)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (4.41.1)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from h5py) (1.15.0)\n","Requirement already satisfied: numpy>=1.7 in /usr/local/lib/python3.6/dist-packages (from h5py) (1.18.5)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"9AeIbG2iu8y1"},"source":["## Data structure\n","\n","To save your time, we've already vectorized all MSCOCO17 images with a pre-trained VGG19 network from [torchvision](https://github.com/pytorch/vision/blob/master/torchvision/models/vgg.py).\n","\n","The whole process takes anywhere between a day on CPU and 30min on 1x GeForce GTX 1060. If you want to play with that yourself, [you're welcome](https://github.com/nadiinchi/dl_labs/blob/master/ht_attention_preprocess_data.ipynb).\n","\n","Please either download data from [here](https://yadi.sk/d/KGh1SngBWH4stg) or generate it manually using the above script."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pkmW474JOlpB","executionInfo":{"status":"ok","timestamp":1606226002565,"user_tz":-180,"elapsed":36473,"user":{"displayName":"Максим Евгеньевич Христолюбов","photoUrl":"","userId":"07722095761291801978"}},"outputId":"cd4e2703-0d95-4ac6-b867-098a79dd3770"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":6,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"YxKsaNQKdObw","executionInfo":{"status":"ok","timestamp":1606226002566,"user_tz":-180,"elapsed":36472,"user":{"displayName":"Максим Евгеньевич Христолюбов","photoUrl":"","userId":"07722095761291801978"}}},"source":["attention_path = '/content/drive/MyDrive/Colab Notebooks/Глубокое обучение/attention/'"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"3JtYSbtHu8y1","executionInfo":{"status":"ok","timestamp":1606226004800,"user_tz":-180,"elapsed":38701,"user":{"displayName":"Максим Евгеньевич Христолюбов","photoUrl":"","userId":"07722095761291801978"}}},"source":["f = h5py.File(attention_path + 'img_codes.hdf5', 'r')\n","img_codes = f['data']"],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"id":"kk_dcZAmu8y1","executionInfo":{"status":"ok","timestamp":1606226006191,"user_tz":-180,"elapsed":40090,"user":{"displayName":"Максим Евгеньевич Христолюбов","photoUrl":"","userId":"07722095761291801978"}}},"source":["captions = json.load(open(attention_path + 'captions_tokenized.json'))"],"execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"id":"LP5JrplNu8y1","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1606226007251,"user_tz":-180,"elapsed":41142,"user":{"displayName":"Максим Евгеньевич Христолюбов","photoUrl":"","userId":"07722095761291801978"}},"outputId":"9fc226ca-2eca-4dc5-e0d4-ff488514f6a0"},"source":["print(\"Each image code is a 512x9x9-unit tensor [ shape: %s x %s ]\" % (str(len(img_codes)), str(img_codes[0].shape)))\n","print(img_codes[0][:3].round(2), end='\\n\\n')\n","print(\"For each image there are 5 reference captions, e.g.:\\n\")\n","print('\\n'.join(captions[0]))"],"execution_count":10,"outputs":[{"output_type":"stream","text":["Each image code is a 512x9x9-unit tensor [ shape: 118287 x (512, 9, 9) ]\n","[[[0.   0.   0.   0.   0.07 0.69 0.   0.   0.  ]\n","  [0.   0.24 0.   0.   0.63 1.94 1.21 0.72 0.46]\n","  [0.77 0.76 0.   0.   0.   0.88 1.65 2.37 1.14]\n","  [0.21 0.   0.   0.   2.8  3.3  2.93 6.29 3.48]\n","  [0.   0.   0.   0.   3.29 3.81 2.24 2.76 0.01]\n","  [0.   0.   0.   1.18 0.32 0.08 0.   0.   0.  ]\n","  [1.02 1.76 0.89 1.77 0.   0.   0.   0.   0.  ]\n","  [0.   1.36 0.   0.   0.   0.   0.   0.   0.  ]\n","  [0.   0.   0.   0.   0.   0.   0.09 0.3  0.  ]]\n","\n"," [[0.   0.   0.   0.   0.   0.   0.   0.   0.  ]\n","  [0.   0.   0.   0.   0.   0.   0.   0.   0.  ]\n","  [0.   0.   0.   0.17 0.   0.   0.   0.   0.  ]\n","  [0.   0.95 0.   0.   0.   0.   0.   0.   0.  ]\n","  [0.34 1.5  0.   0.   0.36 0.   0.   0.   0.  ]\n","  [0.   0.   0.   0.   0.   0.23 0.   0.   0.  ]\n","  [0.   0.   0.   0.   0.   0.52 0.   0.   0.  ]\n","  [0.   0.34 0.   0.   0.   0.   0.86 0.34 0.  ]\n","  [0.   0.47 0.26 0.41 0.   0.   1.08 0.38 0.  ]]\n","\n"," [[1.09 3.03 2.15 0.65 0.   0.   1.06 0.25 0.31]\n","  [2.03 2.99 2.37 0.16 0.27 0.   0.   0.94 1.5 ]\n","  [0.   0.44 0.   0.   0.32 0.   0.   0.   0.  ]\n","  [0.   0.   0.   0.   0.   0.   0.   0.   0.  ]\n","  [0.   0.   0.   0.   0.3  2.91 2.38 0.   0.  ]\n","  [0.   0.55 0.81 1.15 2.72 3.6  2.19 0.24 0.  ]\n","  [0.   0.2  0.67 1.46 2.49 2.69 2.51 1.88 1.88]\n","  [1.99 2.08 0.   0.   0.55 1.03 1.1  2.28 3.31]\n","  [1.64 1.13 0.   0.   0.   0.91 0.96 1.12 0.95]]]\n","\n","For each image there are 5 reference captions, e.g.:\n","\n","closeup of bins of food that include broccoli and bread .\n","a meal is presented in brightly colored plastic trays .\n","there are containers filled with different kinds of foods\n","colorful dishes holding meat , vegetables , fruit , and bread .\n","a bunch of trays that have different food .\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"lAAtBz2du8y1"},"source":["As you can see, all captions are already tokenized and lowercased. We now want to split them and add some special tokens for start/end of caption."]},{"cell_type":"code","metadata":{"id":"i92wL10Ku8y2","executionInfo":{"status":"ok","timestamp":1606226010575,"user_tz":-180,"elapsed":44464,"user":{"displayName":"Максим Евгеньевич Христолюбов","photoUrl":"","userId":"07722095761291801978"}}},"source":["#split descriptions into tokens\n","for img_i in range(len(captions)):\n","    for caption_i in range(len(captions[img_i])):\n","        sentence = captions[img_i][caption_i] \n","        captions[img_i][caption_i] = [\"#START#\"]+sentence.split(' ')+[\"#END#\"]"],"execution_count":11,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KJfsswswu8y2"},"source":["You don't want your network to predict a million-size vector of probabilities at each step, so we're gotta make some cuts. \n","\n","We want you to __count the occurences of each word__ so that we can decide which words to keep in our vocabulary."]},{"cell_type":"code","metadata":{"id":"_DoKglrhu8y2","executionInfo":{"status":"ok","timestamp":1606226012479,"user_tz":-180,"elapsed":46366,"user":{"displayName":"Максим Евгеньевич Христолюбов","photoUrl":"","userId":"07722095761291801978"}}},"source":["# Build a Vocabulary\n","from collections import Counter\n","word_counts = Counter()\n","\n","#Compute word frequencies for each word in captions. See code above for data structure\n","for img_i in range(len(captions)):\n","    for caption_i in range(len(captions[img_i])):\n","      word_counts.update(captions[img_i][caption_i])"],"execution_count":12,"outputs":[]},{"cell_type":"code","metadata":{"id":"_cqO6Q8Nu8y2","executionInfo":{"status":"ok","timestamp":1606226012481,"user_tz":-180,"elapsed":46365,"user":{"displayName":"Максим Евгеньевич Христолюбов","photoUrl":"","userId":"07722095761291801978"}}},"source":["vocab  = ['#UNK#', '#START#', '#END#', '#PAD#']\n","vocab += [k for k, v in word_counts.items() if v >= 5 if k not in vocab]\n","n_tokens = len(vocab)\n","\n","assert 10000 <= n_tokens <= 10500\n","\n","word_to_index = {w: i for i, w in enumerate(vocab)}"],"execution_count":13,"outputs":[]},{"cell_type":"code","metadata":{"id":"Js51Zvs5u8y2","executionInfo":{"status":"ok","timestamp":1606226012482,"user_tz":-180,"elapsed":46365,"user":{"displayName":"Максим Евгеньевич Христолюбов","photoUrl":"","userId":"07722095761291801978"}}},"source":["eos_ix = word_to_index['#END#']\n","unk_ix = word_to_index['#UNK#']\n","pad_ix = word_to_index['#PAD#']\n","\n","def as_matrix(sequences, max_len=None):\n","    \"\"\" Convert a list of tokens into a matrix with padding \"\"\"\n","    max_len = max_len or max(map(len,sequences))\n","    \n","    matrix = np.zeros((len(sequences), max_len), dtype='int32') + pad_ix\n","    for i,seq in enumerate(sequences):\n","        row_ix = [word_to_index.get(word, unk_ix) for word in seq[:max_len]]\n","        matrix[i, :len(row_ix)] = row_ix\n","    \n","    return matrix"],"execution_count":14,"outputs":[]},{"cell_type":"code","metadata":{"id":"eUslnA8Ru8y2","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1606226012482,"user_tz":-180,"elapsed":46357,"user":{"displayName":"Максим Евгеньевич Христолюбов","photoUrl":"","userId":"07722095761291801978"}},"outputId":"166ceb75-3cf6-4056-a04d-d1d3d84aa42b"},"source":["#try it out on several descriptions of a random image\n","as_matrix(captions[1337])"],"execution_count":15,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[   1,   14,  287,  141,   14, 1130,  256,   62,   14, 1291,    2,\n","           3,    3,    3],\n","       [   1,   14,  287,   62,   14, 1130,  256,  141,   14, 1291,    2,\n","           3,    3,    3],\n","       [   1,   14,   90,  141,   14, 1291,   62,   44,    5,   14, 1287,\n","          13,    2,    3],\n","       [   1,   14, 1238, 1370,   14, 1291,   62,   14,   73,   74,   13,\n","           2,    3,    3],\n","       [   1,   14,   90,   18,   71,  342,  141,   14, 1291,   62,   14,\n","        1287,   13,    2]], dtype=int32)"]},"metadata":{"tags":[]},"execution_count":15}]},{"cell_type":"markdown","metadata":{"id":"wFg64bTku8y2"},"source":["## Building our neural network\n","\n","As we mentioned earlier, we shall build an rnn \"language-model\" conditioned on the features from the convolutional part. \n","\n","We'll unbox the inception net later to save memory, for now just pretend that it's available."]},{"cell_type":"code","metadata":{"id":"Hppeixwfu8y2","executionInfo":{"status":"ok","timestamp":1606226015962,"user_tz":-180,"elapsed":49835,"user":{"displayName":"Максим Евгеньевич Христолюбов","photoUrl":"","userId":"07722095761291801978"}}},"source":["import torch, torch.nn as nn\n","import torch.nn.functional as F"],"execution_count":16,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SycorgCxu8y2"},"source":["### Attention\n","\n","There are $K$ objects that you can pay attention to.\n","Each object is characterized by the key $k_i$ and the value $v_i$.\n","The attention layer proceeds queries.\n","For the query $q$, the layer returns a weighted sum of the values of the objects, with weights proportional to the degree of key matching the query:\n","$$w_i = \\frac{\\exp(score(q, k_i))}{\\sum_{j=1}^K\\exp(score(q, k_j))}$$\n","$$a = \\sum_{i=1}^K w_i v_i$$\n","\n","Here we use $score(q, k) = \\frac{q^Tk}{\\sqrt{dim(k)}}$, where $dim(k)$ is the dimensionality of the key (which also equals the dimensionality of the query).\n","For more information see the paper Vaswani et al. \"Attention Is All You Need\", 2017.\n","\n","_Hint:_ It is recommended to pay attention to the function torch.bmm, it may be useful below."]},{"cell_type":"markdown","metadata":{"id":"pu9mhi4-u8y2"},"source":["#### Score function layer"]},{"cell_type":"code","metadata":{"id":"Lpa9jGOzu8y2","executionInfo":{"status":"ok","timestamp":1606226015964,"user_tz":-180,"elapsed":49835,"user":{"displayName":"Максим Евгеньевич Христолюбов","photoUrl":"","userId":"07722095761291801978"}}},"source":["class ScaledDotProductScore(nn.Module):\n","    \"\"\"\n","    Vaswani et al. \"Attention Is All You Need\", 2017.\n","    \"\"\"\n","    def __init__(self):\n","        super().__init__()\n","\n","    def forward(self, queries, keys):\n","        \"\"\"\n","        queries:  [batch_size x num_queries x dim]\n","        keys:     [batch_size x num_objects x dim]\n","        Returns a tensor of scores with shape [batch_size x num_queries x num_objects].\n","        \"\"\"\n","        dim = keys.size()[2]\n","        result = torch.bmm(queries, torch.transpose(keys, 1, 2))/np.sqrt(dim)\n","        return result"],"execution_count":17,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"B7yEFsIzu8y2"},"source":["Test for ScaledDotProductScore"]},{"cell_type":"code","metadata":{"id":"rB9BnK92u8y2","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1606226015966,"user_tz":-180,"elapsed":49831,"user":{"displayName":"Максим Евгеньевич Христолюбов","photoUrl":"","userId":"07722095761291801978"}},"outputId":"8efcdd39-350f-4bac-bbe3-57ec584931b0"},"source":["q = torch.tensor([[\n","    [0, 0, 0, 0, 1],\n","    [0, 0, 1, 0, 0],\n","    [1, 0, 0, 0, 0],\n","]], dtype=torch.float32)\n","o = torch.tensor([[\n","    [0, 0, 0, 0, 1],\n","    [0, 0, 0, 1, 0],\n","    [0, 0, 1, 0, 0],\n","    [0, 1, 0, 0, 0],\n","]], dtype=torch.float32)\n","print(ScaledDotProductScore()(q, o))"],"execution_count":18,"outputs":[{"output_type":"stream","text":["tensor([[[0.4472, 0.0000, 0.0000, 0.0000],\n","         [0.0000, 0.0000, 0.4472, 0.0000],\n","         [0.0000, 0.0000, 0.0000, 0.0000]]])\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"KWsXGH34u8y2"},"source":["#### Attention layer"]},{"cell_type":"code","metadata":{"id":"x9lav1STu8y2","executionInfo":{"status":"ok","timestamp":1606226015966,"user_tz":-180,"elapsed":49829,"user":{"displayName":"Максим Евгеньевич Христолюбов","photoUrl":"","userId":"07722095761291801978"}}},"source":["class Attention(nn.Module):\n","    def __init__(self, scorer):\n","        super().__init__()\n","        self.scorer = scorer\n","\n","    def forward(self, queries, keys, values):\n","        \"\"\"\n","        queries:         [batch_size x num_queries x query_feature_dim]\n","        keys:            [batch_size x num_objects x key_feature_dim]\n","        values:          [batch_size x num_objects x obj_feature_dim]\n","        Returns matrix of responses for queries with shape [batch_size x num_queries x obj_feature_dim].\n","        Saves detached weights as self.attention_map.\n","        \"\"\"\n","        scores = self.scorer(queries, keys)\n","        weights = F.softmax(scores, dim=2)\n","        self.attention_map = weights.detach()\n","        self.last_weights = self.attention_map\n","        result = torch.bmm(weights, values)\n","        return result"],"execution_count":19,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7syUpe4_u8y2"},"source":["Tests for Attention layer"]},{"cell_type":"code","metadata":{"id":"mp-PO9Ksu8y2","executionInfo":{"status":"ok","timestamp":1606226015967,"user_tz":-180,"elapsed":49828,"user":{"displayName":"Максим Евгеньевич Христолюбов","photoUrl":"","userId":"07722095761291801978"}}},"source":["attn = Attention(ScaledDotProductScore())"],"execution_count":20,"outputs":[]},{"cell_type":"code","metadata":{"id":"tqlMMUMyu8y2","executionInfo":{"status":"ok","timestamp":1606226015967,"user_tz":-180,"elapsed":49825,"user":{"displayName":"Максим Евгеньевич Христолюбов","photoUrl":"","userId":"07722095761291801978"}}},"source":["q = torch.randn(2, 3, 5)\n","k = torch.randn(2, 4, 5)\n","v = torch.randn(2, 4, 7)\n","assert attn(q, k, v).shape == (2, 3, 7)"],"execution_count":21,"outputs":[]},{"cell_type":"code","metadata":{"id":"YDoTJ-7su8y2","executionInfo":{"status":"ok","timestamp":1606226015968,"user_tz":-180,"elapsed":49822,"user":{"displayName":"Максим Евгеньевич Христолюбов","photoUrl":"","userId":"07722095761291801978"}}},"source":["q = torch.tensor([[\n","    [0.01],\n","    [1],\n","    [100],\n","]], dtype=torch.float32)\n","o = torch.tensor([[\n","    [-1],\n","    [0],\n","    [1],\n","]], dtype=torch.float32) * 1000\n","a = attn(q, o, o)\n","assert torch.isnan(attn.last_weights).sum() == 0\n","assert torch.isnan(a).sum() == 0"],"execution_count":22,"outputs":[]},{"cell_type":"code","metadata":{"id":"8YKI0c31u8y2","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1606226015968,"user_tz":-180,"elapsed":49815,"user":{"displayName":"Максим Евгеньевич Христолюбов","photoUrl":"","userId":"07722095761291801978"}},"outputId":"9c40a8a2-9681-452f-c1f9-ab82ed45a15c"},"source":["q = torch.tensor([[\n","    [0, 1],\n","    [1, 0],\n","]], dtype=torch.float32)\n","k = torch.tensor([[\n","    [0, 0],\n","    [0, 1],\n","    [1, 0],\n","]], dtype=torch.float32)\n","v = torch.tensor([[\n","    [0],\n","    [1],\n","    [2],\n","]], dtype=torch.float32)\n","a = attn(q, k, v)\n","print('Attention map:\\n', attn.last_weights)\n","print('Responses:\\n', a)"],"execution_count":23,"outputs":[{"output_type":"stream","text":["Attention map:\n"," tensor([[[0.2483, 0.5035, 0.2483],\n","         [0.2483, 0.2483, 0.5035]]])\n","Responses:\n"," tensor([[[1.0000],\n","         [1.2552]]])\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"QIJ4njfWu8y2"},"source":["### Language model"]},{"cell_type":"code","metadata":{"id":"LBqrNGuLu8y2","executionInfo":{"status":"ok","timestamp":1606250795801,"user_tz":-180,"elapsed":1005,"user":{"displayName":"Максим Евгеньевич Христолюбов","photoUrl":"","userId":"07722095761291801978"}}},"source":["class CaptionNet(nn.Module):\n","    def __init__(self, n_tokens=n_tokens, emb_size=128, lstm_units=256, cnn_channels=512, device = None):\n","        \"\"\" A recurrent 'head' network for image captioning. Read scheme below. \"\"\"\n","        super(self.__class__, self).__init__()\n","\n","        self.n_tokens = n_tokens\n","        self.emb_size = emb_size\n","        self.lstm_units = lstm_units\n","        self.cnn_channels = cnn_channels\n","        self.device = device\n","        \n","        # a layer that converts conv features to \n","        self.cnn_to_h0 = nn.Linear(self.cnn_channels, self.lstm_units)\n","        self.cnn_to_c0 = nn.Linear(self.cnn_channels, self.lstm_units)\n","        \n","        # recurrent part, please create the layers as per scheme above.\n","\n","        # create embedding for input words. Use the parameters (e.g. emb_size).\n","        self.emb = nn.Embedding(self.n_tokens, self.emb_size, padding_idx = pad_ix, max_norm = 2) \n","            \n","        # attention: create attention over image spatial positions\n","        # The query is previous lstm hidden state, the keys are transformed cnn features,\n","        # the values are cnn features\n","        self.attention = Attention(ScaledDotProductScore())\n","        \n","        # attention: create transform from cnn features to the keys\n","        # Hint: one linear layer shoud work\n","        # Hint: the dimensionality of keys should be lstm_units as lstm\n","        #       hidden state is the attention query\n","        self.cnn_to_attn_key = nn.Linear(self.cnn_channels, self.lstm_units)\n","                \n","        # lstm: create a recurrent core of your network. Use LSTMCell\n","        self.lstm = nn.LSTMCell(self.cnn_channels +self. emb_size, self.lstm_units)\n","\n","        # create logits: MLP that takes attention response, lstm hidden state\n","        # and the previous word embedding as an input and computes one number per token\n","        # Hint: I used an architecture with one hidden layer, but you may try deeper ones\n","        self.logits_mlp = nn.Sequential(nn.Linear(self.lstm_units + self.emb_size + self.cnn_channels, self.emb_size), \n","                                        nn.ReLU(),\n","                                        nn.Dropout(p = 0.001),\n","                                        nn.Linear(self.emb_size, self.n_tokens),\n","                                        nn.ReLU())\n","\n","    def reccurent_step(self, prev_hid, prev_cell, attn_keys, image_features, captions_emb):\n","        query = prev_hid.unsqueeze(dim=1)\n","        response = self.attention(query, attn_keys, image_features).squeeze(1)\n","        response_сoncat = torch.cat((response, captions_emb), dim=1)\n","        cur_hid, cur_cell = self.lstm(response_сoncat, (prev_hid, prev_cell))\n","\n","        attn_map = self.attention.attention_map.squeeze(1)\n","\n","        return torch.cat((cur_hid, captions_emb, response), dim=1), attn_map, cur_hid, cur_cell\n","\n","        \n","    def forward(self, image_features, captions_ix):\n","        \"\"\" \n","        Apply the network in training mode. \n","        :param image_features: torch tensor containing VGG features for each position.\n","                               shape: [batch, cnn_channels, width * height]\n","        :param captions_ix: torch tensor containing captions as matrix. shape: [batch, word_i]. \n","            padded with pad_ix\n","        :returns: logits for next token at each tick, shape: [batch, word_i, n_tokens]\n","        \"\"\"\n","        image_features = image_features.to(self.device)\n","        captions_ix = captions_ix.to(self.device)\n","\n","        initial_cell = self.cnn_to_c0(image_features.mean(2))\n","        initial_hid = self.cnn_to_h0(image_features.mean(2))\n","        \n","        image_features = image_features.transpose(1, 2)\n","        \n","        # compute embeddings for captions_ix\n","        captions_emb = self.emb(captions_ix)\n","        \n","        # apply recurrent layer to captions_emb.\n","        # 1. initialize lstm state with initial_* from above\n","        # 2. In the recurrent loop over tokens:\n","        #   2.1. transform image vectors to the keys for attention\n","        #   2.2. use previous lstm state as an attention query and image vectors as values\n","        #   2.3. apply attention to obtain context vector\n","        #   2.4. store attention map\n","        #   2.5. feed lstm with current token embedding concatenated with context vector\n","        #   2.6. update lstm hidden and cell vectors\n","        #   2.7. store current lstm hidden state, attention response, and the previous word embedding\n","        # reccurent_out should be lstm hidden state sequence\n","        # of shape [batch, caption_length, lstm_units + cnn_channels + emb_size]\n","        # attention_map should be attention maps sequence\n","        # of shape [batch, caption_length, width * height]\n","\n","        recurrent_out = torch.zeros((captions_ix.shape[0], captions_ix.shape[1], self.lstm_units + self.emb_size + self.cnn_channels)).to(self.device)\n","        attention_map = torch.zeros((captions_ix.shape[0], captions_ix.shape[1], 9*9)).to(self.device)\n","        \n","        attn_keys = self.cnn_to_attn_key(image_features)\n","        prev_hid = initial_hid\n","        prev_cell = initial_cell\n","        for i in range(captions_ix.shape[1]):\n","          recurrent_out[:, i, :],  attention_map[:,i,:], prev_hid, prev_cell = self.reccurent_step(prev_hid, prev_cell, attn_keys, image_features, captions_emb[:, i, :])\n","\n","        # compute logits for next token probabilities\n","        # based on the stored in (2.7) values (reccurent_out)\n","        logits = self.logits_mlp(recurrent_out)\n","        \n","        # return logits and attention maps from (2.4)\n","        return logits, attention_map\n","\n","    def forward(self, image_features, captions_ix):\n","        \"\"\" \n","        Apply the network in training mode. \n","        :param image_features: torch tensor containing VGG features for each position.\n","                               shape: [batch, cnn_channels, width * height]\n","        :param captions_ix: torch tensor containing captions as matrix. shape: [batch, word_i]. \n","            padded with pad_ix\n","        :returns: logits for next token at each tick, shape: [batch, word_i, n_tokens]\n","        \"\"\"\n","        image_features = image_features.to(self.device)\n","        captions_ix = captions_ix.to(self.device)\n","\n","        initial_cell = self.cnn_to_c0(image_features.mean(2))\n","        initial_hid = self.cnn_to_h0(image_features.mean(2))\n","        \n","        image_features = image_features.transpose(1, 2)\n","        \n","        # compute embeddings for captions_ix\n","        captions_emb = self.emb(captions_ix)\n","        \n","        # apply recurrent layer to captions_emb.\n","        # 1. initialize lstm state with initial_* from above\n","        # 2. In the recurrent loop over tokens:\n","        #   2.1. transform image vectors to the keys for attention\n","        #   2.2. use previous lstm state as an attention query and image vectors as values\n","        #   2.3. apply attention to obtain context vector\n","        #   2.4. store attention map\n","        #   2.5. feed lstm with current token embedding concatenated with context vector\n","        #   2.6. update lstm hidden and cell vectors\n","        #   2.7. store current lstm hidden state, attention response, and the previous word embedding\n","        # reccurent_out should be lstm hidden state sequence\n","        # of shape [batch, caption_length, lstm_units + cnn_channels + emb_size]\n","        # attention_map should be attention maps sequence\n","        # of shape [batch, caption_length, width * height]\n","\n","        recurrent_out = []\n","        attention_map = []\n","        \n","        attn_keys = self.cnn_to_attn_key(image_features)\n","        prev_hid = initial_hid\n","        prev_cell = initial_cell\n","        for i in range(captions_ix.shape[1]):\n","          recurrent_out1,  attention_map1, prev_hid, prev_cell = self.reccurent_step(prev_hid, prev_cell, attn_keys, image_features, captions_emb[:, i, :])\n","          recurrent_out.append(recurrent_out1.unsqueeze(dim=1))\n","          attention_map.append(attention_map1.unsqueeze(dim=1))\n","\n","        recurrent_out = torch.cat(recurrent_out, dim=1)\n","        attention_map = torch.cat(attention_map, dim=1)\n","\n","        # compute logits for next token probabilities\n","        # based on the stored in (2.7) values (reccurent_out)\n","        logits = self.logits_mlp(recurrent_out)\n","        \n","        # return logits and attention maps from (2.4)\n","        return logits, attention_map"],"execution_count":417,"outputs":[]},{"cell_type":"code","metadata":{"id":"6ZE99m3Pu8y2","executionInfo":{"status":"ok","timestamp":1606250799691,"user_tz":-180,"elapsed":939,"user":{"displayName":"Максим Евгеньевич Христолюбов","photoUrl":"","userId":"07722095761291801978"}}},"source":["network = CaptionNet(n_tokens, device = 'cuda')\n","network = network.to(network.device)"],"execution_count":418,"outputs":[]},{"cell_type":"code","metadata":{"id":"MnRCIfkgu8y2","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1606248802026,"user_tz":-180,"elapsed":1349,"user":{"displayName":"Максим Евгеньевич Христолюбов","photoUrl":"","userId":"07722095761291801978"}},"outputId":"d56a8722-08b8-42d2-a00a-4d1374ced242"},"source":["dummy_img_vec = torch.randn(len(captions[0]), 512, 81)\n","dummy_capt_ix = torch.tensor(as_matrix(captions[0]), dtype=torch.int64)\n","\n","dummy_logits, dummy_attention_map = network.forward(dummy_img_vec, dummy_capt_ix)\n","dummy_logits, dummy_attention_map = network.forward(dummy_img_vec, dummy_capt_ix)\n","\n","print('logits shape:', dummy_logits.shape)\n","print('attention map shape:', dummy_attention_map.shape)\n","assert dummy_logits.shape == (dummy_capt_ix.shape[0], dummy_capt_ix.shape[1], n_tokens)\n","assert dummy_attention_map.shape == (dummy_capt_ix.shape[0], dummy_capt_ix.shape[1], 81)"],"execution_count":385,"outputs":[{"output_type":"stream","text":["logits shape: torch.Size([5, 14, 10403])\n","attention map shape: torch.Size([5, 14, 81])\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"pyLf23hlu8y2"},"source":["#### Train loss function"]},{"cell_type":"code","metadata":{"id":"Ai3PXTgHu8y2","executionInfo":{"status":"ok","timestamp":1606248802514,"user_tz":-180,"elapsed":1006,"user":{"displayName":"Максим Евгеньевич Христолюбов","photoUrl":"","userId":"07722095761291801978"}}},"source":["def compute_loss(network, image_features, captions_ix):\n","    \"\"\"\n","    :param image_features: torch tensor containing VGG features. shape: [batch, cnn_channels, width * height]\n","    :param captions_ix: torch tensor containing captions as matrix. shape: [batch, word_i]. \n","        padded with pad_ix\n","    :returns: crossentropy (neg llh) loss for next captions_ix given previous ones plus \n","              attention regularizer. Scalar float tensor\n","    \"\"\"\n","    \n","    if next(network.parameters()).is_cuda:\n","        image_features, captions_ix = image_features.cuda(), captions_ix.cuda()\n","    \n","    # captions for input - all except last cuz we don't know next token for last one.\n","    captions_ix_inp = captions_ix[:, :-1].contiguous()\n","    captions_ix_next = captions_ix[:, 1:].contiguous()\n","    \n","    # apply the network, get predictions, attnetion map and gates for captions_ix_next\n","    logits_for_next, attention_map = network.forward(image_features, captions_ix_inp)\n","    \n","    \n","    # compute the loss function between logits_for_next and captions_ix_next\n","    # Use the mask, Luke: make sure that predicting next tokens after EOS do not contribute to loss\n","    # you can do that either by multiplying elementwise loss by (captions_ix_next != pad_ix)\n","    # or by using ignore_index in some losses.\n","    \n","    loss = nn.CrossEntropyLoss(ignore_index = pad_ix)\n","\n","    # the regularizer for attention - this one requires the attention over each position to sum up to 1,\n","    # i. e. to look at the whole image during sentence generation process\n","    mask = (captions_ix_inp != pad_ix)\n","    masked_attention_map = attention_map * mask[:, :, None].float()\n","    regularizer = ((1 - masked_attention_map.sum(1)) ** 2).mean()\n","    \n","    return loss(logits_for_next.permute(0, 2, 1), captions_ix_next) + regularizer"],"execution_count":386,"outputs":[]},{"cell_type":"code","metadata":{"id":"iYh-2Y0-u8y2","executionInfo":{"status":"ok","timestamp":1606248802988,"user_tz":-180,"elapsed":785,"user":{"displayName":"Максим Евгеньевич Христолюбов","photoUrl":"","userId":"07722095761291801978"}}},"source":["dummy_loss = compute_loss(network, dummy_img_vec, dummy_capt_ix)\n","\n","assert len(dummy_loss.shape) <= 1, 'loss must be scalar'\n","assert dummy_loss.detach().cpu().numpy() > 0, \"did you forget the 'negative' part of negative log-likelihood\"\n","\n","dummy_loss.backward()\n","\n","assert all(param.grad is not None for param in network.parameters()), \\\n","        'loss should depend differentiably on all neural network weights'"],"execution_count":387,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"veOOKnocu8y2"},"source":["#### Optimizer\n","Create ~~adam~~ your favorite optimizer for the network."]},{"cell_type":"code","metadata":{"id":"PC816TRku8y2","executionInfo":{"status":"ok","timestamp":1606252996051,"user_tz":-180,"elapsed":952,"user":{"displayName":"Максим Евгеньевич Христолюбов","photoUrl":"","userId":"07722095761291801978"}}},"source":["import torch.optim as optim\n","\n","optimizer = optim.Adam(network.parameters(), lr=0.003, weight_decay = 1*10**(-8))\n","CLIP = 2\n","\n","scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1, gamma=1.)"],"execution_count":430,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gLKRTwhFu8y2"},"source":["# Training\n","\n","* First make train/val split without extra memory usage\n","* Implement the batch generator\n","* Than train the network as usual"]},{"cell_type":"code","metadata":{"id":"NDQsLuB8u8y2","executionInfo":{"status":"ok","timestamp":1606252996578,"user_tz":-180,"elapsed":1015,"user":{"displayName":"Максим Евгеньевич Христолюбов","photoUrl":"","userId":"07722095761291801978"}}},"source":["from torch.utils.data import Dataset\n","\n","class IdxDataset(Dataset):\n","    def __init__(self, dataset, idx):\n","        self.dataset = dataset\n","        self.idx = idx\n","    \n","    def __len__(self):\n","        return len(self.idx)\n","        \n","    def __getitem__(self, idx):\n","        return self.dataset[self.idx[idx]]"],"execution_count":431,"outputs":[]},{"cell_type":"code","metadata":{"id":"f7YohOzIu8y2","executionInfo":{"status":"ok","timestamp":1606252997032,"user_tz":-180,"elapsed":1279,"user":{"displayName":"Максим Евгеньевич Христолюбов","photoUrl":"","userId":"07722095761291801978"}}},"source":["captions = np.array(captions)\n","\n","np.random.seed(42)\n","perm = np.random.permutation(len(img_codes))\n","threshold = round(len(img_codes) * 0.1)\n","train_img_idx, val_img_idx = perm[threshold:], perm[: threshold]\n","\n","train_img_idx.sort()\n","val_img_idx.sort()\n","train_img_codes = IdxDataset(img_codes, train_img_idx)\n","val_img_codes = IdxDataset(img_codes, val_img_idx)\n","train_captions = IdxDataset(captions, train_img_idx)\n","val_captions = IdxDataset(captions, val_img_idx)"],"execution_count":432,"outputs":[]},{"cell_type":"code","metadata":{"id":"z47jzdCpu8y2","executionInfo":{"status":"ok","timestamp":1606252997032,"user_tz":-180,"elapsed":1057,"user":{"displayName":"Максим Евгеньевич Христолюбов","photoUrl":"","userId":"07722095761291801978"}}},"source":["from random import choice\n","\n","last_batch_end = {}\n","\n","def generate_batch(img_codes, captions, batch_size, max_caption_len=None):\n","    \n","    #sample sequential numbers for image/caption indicies (for trainign speed up)\n","    global last_batch_end\n","    random_image_ix = np.arange(batch_size, dtype='int') + last_batch_end.get(len(img_codes), 0)\n","    last_batch_end[len(img_codes)] = last_batch_end.get(len(img_codes), 0) + batch_size\n","    if last_batch_end[len(img_codes)] + batch_size >= len(img_codes):\n","        last_batch_end[len(img_codes)] = 0\n","    \n","    #get images\n","    batch_images = np.vstack([img_codes[i][None] for i in random_image_ix])\n","    batch_images = batch_images.reshape(batch_images.shape[0], batch_images.shape[1], -1)\n","    \n","    #5-7 captions for each image\n","    captions_for_batch_images = captions[random_image_ix]\n","    \n","    #pick one from a set of captions for each image\n","    batch_captions = list(map(choice,captions_for_batch_images))\n","    \n","    #convert to matrix\n","    batch_captions_ix = as_matrix(batch_captions,max_len=max_caption_len)\n","    \n","    return torch.tensor(batch_images, dtype=torch.float32), torch.tensor(batch_captions_ix, dtype=torch.int64)"],"execution_count":433,"outputs":[]},{"cell_type":"code","metadata":{"id":"lmMXD7rau8y2","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1606252997033,"user_tz":-180,"elapsed":590,"user":{"displayName":"Максим Евгеньевич Христолюбов","photoUrl":"","userId":"07722095761291801978"}},"outputId":"32758afd-282d-441a-8fdc-6d9c979803f7"},"source":["generate_batch(img_codes, captions, 3)"],"execution_count":434,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(tensor([[[0.0000, 0.0000, 0.0000,  ..., 0.0941, 0.2962, 0.0000],\n","          [0.0000, 0.0000, 0.0000,  ..., 1.0764, 0.3785, 0.0000],\n","          [1.0881, 3.0323, 2.1466,  ..., 0.9595, 1.1179, 0.9478],\n","          ...,\n","          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n","          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n","          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n"," \n","         [[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n","          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n","          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n","          ...,\n","          [0.0000, 0.0000, 0.0711,  ..., 0.0000, 0.0000, 0.0000],\n","          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n","          [0.0000, 0.0000, 0.0000,  ..., 1.1465, 1.6479, 1.6033]],\n"," \n","         [[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.3930, 0.9311],\n","          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n","          [0.0000, 0.0000, 0.0000,  ..., 0.4969, 0.0000, 0.2054],\n","          ...,\n","          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n","          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n","          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]]]),\n"," tensor([[ 1, 14, 38,  5, 22,  8, 39, 28,  7, 13,  2],\n","         [ 1, 14, 40, 46, 47, 48, 14, 45,  2,  3,  3],\n","         [ 1, 14, 65, 60, 26, 27, 28, 20, 66, 13,  2]]))"]},"metadata":{"tags":[]},"execution_count":434}]},{"cell_type":"markdown","metadata":{"id":"R7oboCT5u8y2"},"source":["### Main loop\n","\n","Train on minibatches just as usual. Evaluate on val from time to time.\n","\n","##### TIps\n","* If training loss has become close to 0 or model produces garbage,\n","    double-check that you're predicting __next__ words, not current or t+2'th words.\n","* If the model generates fluent captions that have nothing to do with the images\n"," * this may be due to recurrent net not receiving image vectors.\n"," * alternatively it may be caused by gradient explosion, try clipping 'em or just restarting the training\n"," * finally, you may just need to train the model a bit more\n","\n","\n","* Crossentropy is a poor measure of overfitting\n"," * Model can overfit validation crossentropy but keep improving validation quality.\n"," * Use human _(manual)_ evaluation or try automated metrics: [cider](https://github.com/vrama91/cider) or [bleu](https://www.nltk.org/_modules/nltk/translate/bleu_score.html)\n"," \n","\n","* We recommend you to periodically evaluate the network using the next \"apply trained model\" block\n"," *  its safe to interrupt training, run a few examples and start training again\n"," \n","* The typical loss values should be around 3~5 if you average over time, scale by length if you sum over time. The reasonable captions began appearing at loss=3.5~3.7"]},{"cell_type":"code","metadata":{"id":"xWTy17VOu8y2","executionInfo":{"status":"ok","timestamp":1606252999166,"user_tz":-180,"elapsed":1573,"user":{"displayName":"Максим Евгеньевич Христолюбов","photoUrl":"","userId":"07722095761291801978"}}},"source":["batch_size = 64  # adjust me\n","n_epochs = 128    # adjust me\n","n_batches_per_epoch = 64  # adjust me\n","n_validation_batches = 16  # how many batches are used for validation after each epoch"],"execution_count":435,"outputs":[]},{"cell_type":"code","metadata":{"id":"vC0G32s-u8y2","colab":{"base_uri":"https://localhost:8080/","height":0},"executionInfo":{"status":"error","timestamp":1606253399115,"user_tz":-180,"elapsed":400923,"user":{"displayName":"Максим Евгеньевич Христолюбов","photoUrl":"","userId":"07722095761291801978"}},"outputId":"94d45378-4a07-4a13-bc65-0ce7ecfd0a92"},"source":["from tqdm import tqdm\n","\n","for epoch in range(n_epochs):\n","    if torch.cuda.is_available():\n","        network = network.cuda()\n","   \n","    train_loss=0\n","    network.train(True)\n","    with tqdm(range(n_batches_per_epoch)) as iterator:\n","        for _ in iterator:\n","            image_features, captions_ix = generate_batch(train_img_codes, train_captions, batch_size)\n","            loss_t = compute_loss(network, image_features, captions_ix)\n","        \n","            # clear old gradients; do a backward pass to get new gradients; then train with opt\n","            optimizer.zero_grad()\n","            loss_t.backward()\n","            torch.nn.utils.clip_grad_norm_(network.parameters(), CLIP)\n","            optimizer.step()\n","        \n","            train_loss += float(loss_t)\n","    train_loss /= n_batches_per_epoch\n","\n","    scheduler.step()\n","        \n","    val_loss = 0\n","    network.train(False)\n","    for _ in range(n_validation_batches):\n","        image_features, captions_ix = generate_batch(train_img_codes, train_captions, batch_size)\n","        loss_t = compute_loss(network, image_features, captions_ix)\n","        val_loss += float(loss_t)\n","    val_loss /= n_validation_batches\n","    \n","    if torch.cuda.is_available():\n","        network = network.cpu()\n","\n","    print('\\nEpoch: {}, train loss: {}, val loss: {}'.format(epoch, train_loss, val_loss), flush=True)\n","\n","print(\"Finished!\")"],"execution_count":436,"outputs":[{"output_type":"stream","text":["100%|██████████| 64/64 [00:18<00:00,  3.52it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["\n","Epoch: 0, train loss: 5.370754107832909, val loss: 5.35842227935791\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 64/64 [00:14<00:00,  4.50it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["\n","Epoch: 1, train loss: 5.366649180650711, val loss: 5.387333124876022\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 64/64 [00:17<00:00,  3.57it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["\n","Epoch: 2, train loss: 5.366041116416454, val loss: 5.355794072151184\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 64/64 [00:03<00:00, 21.00it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["\n","Epoch: 3, train loss: 5.398667149245739, val loss: 5.348285019397736\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 64/64 [00:20<00:00,  3.09it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["\n","Epoch: 4, train loss: 5.37662099301815, val loss: 5.432583421468735\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 64/64 [00:20<00:00,  3.11it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["\n","Epoch: 5, train loss: 5.351512655615807, val loss: 5.388454109430313\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 64/64 [00:20<00:00,  3.06it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["\n","Epoch: 6, train loss: 5.3871031776070595, val loss: 5.452888607978821\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 64/64 [00:21<00:00,  3.02it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["\n","Epoch: 7, train loss: 5.379885919392109, val loss: 5.382055580615997\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 64/64 [00:20<00:00,  3.10it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["\n","Epoch: 8, train loss: 5.368437193334103, val loss: 5.362689316272736\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 64/64 [00:20<00:00,  3.06it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["\n","Epoch: 9, train loss: 5.3687234446406364, val loss: 5.398873507976532\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 64/64 [00:20<00:00,  3.06it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["\n","Epoch: 10, train loss: 5.386172920465469, val loss: 5.299694180488586\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 64/64 [00:20<00:00,  3.06it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["\n","Epoch: 11, train loss: 5.339065559208393, val loss: 5.3780748546123505\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 64/64 [00:20<00:00,  3.08it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["\n","Epoch: 12, train loss: 5.352856248617172, val loss: 5.364471733570099\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 64/64 [00:20<00:00,  3.08it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["\n","Epoch: 13, train loss: 5.369907721877098, val loss: 5.380780845880508\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 64/64 [00:20<00:00,  3.08it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["\n","Epoch: 14, train loss: 5.359851583838463, val loss: 5.4016386568546295\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 64/64 [00:20<00:00,  3.09it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["\n","Epoch: 15, train loss: 5.361455835402012, val loss: 5.3095866441726685\n"],"name":"stdout"},{"output_type":"stream","text":[" 83%|████████▎ | 53/64 [00:20<00:04,  2.57it/s]\n"],"name":"stderr"},{"output_type":"error","ename":"OSError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)","\u001b[0;32m<ipython-input-436-26f139fc9ef9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_batches_per_epoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m             \u001b[0mimage_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptions_ix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_img_codes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_captions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m             \u001b[0mloss_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptions_ix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-433-866eb630c90a>\u001b[0m in \u001b[0;36mgenerate_batch\u001b[0;34m(img_codes, captions, batch_size, max_caption_len)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;31m#get images\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0mbatch_images\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mimg_codes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrandom_image_ix\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0mbatch_images\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_images\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_images\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_images\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-433-866eb630c90a>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;31m#get images\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0mbatch_images\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mimg_codes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrandom_image_ix\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0mbatch_images\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_images\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_images\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_images\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-431-41917b5f1a78>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n","\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/h5py/_hl/dataset.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    571\u001b[0m         \u001b[0mmspace\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5s\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_simple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    572\u001b[0m         \u001b[0mfspace\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mselection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 573\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmspace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfspace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdxpl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dxpl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    574\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    575\u001b[0m         \u001b[0;31m# Patch up the output for NumPy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n","\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n","\u001b[0;32mh5py/h5d.pyx\u001b[0m in \u001b[0;36mh5py.h5d.DatasetID.read\u001b[0;34m()\u001b[0m\n","\u001b[0;32mh5py/_proxy.pyx\u001b[0m in \u001b[0;36mh5py._proxy.dset_rw\u001b[0;34m()\u001b[0m\n","\u001b[0;32mh5py/_proxy.pyx\u001b[0m in \u001b[0;36mh5py._proxy.H5PY_H5Dread\u001b[0;34m()\u001b[0m\n","\u001b[0;31mOSError\u001b[0m: Can't read data (file read failed: time = Tue Nov 24 21:29:58 2020\n, filename = '/content/drive/MyDrive/Colab Notebooks/Глубокое обучение/attention/img_codes.hdf5', file descriptor = 57, errno = 5, error message = 'Input/output error', buf = 0x827ad000, total read size = 3656, bytes this sub-read = 3656, bytes actually read = 18446744073709551615, offset = 15904066920)"]}]},{"cell_type":"code","metadata":{"id":"RtvvnAjmiPb_","executionInfo":{"status":"ok","timestamp":1606250779898,"user_tz":-180,"elapsed":1275,"user":{"displayName":"Максим Евгеньевич Христолюбов","photoUrl":"","userId":"07722095761291801978"}}},"source":["PATH = attention_path + 'network2.ckpt'\n","torch.save(network.state_dict(), PATH)"],"execution_count":416,"outputs":[]},{"cell_type":"code","metadata":{"id":"mioNcWmgiXzk","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1606250862885,"user_tz":-180,"elapsed":958,"user":{"displayName":"Максим Евгеньевич Христолюбов","photoUrl":"","userId":"07722095761291801978"}},"outputId":"4cae7723-c646-4350-e44f-9e6f43d04471"},"source":["PATH = attention_path + 'network2.ckpt'\n","network = CaptionNet(n_tokens, device = 'cuda')\n","network = network.to(network.device)\n","network.load_state_dict(torch.load(PATH))\n","network.eval()"],"execution_count":422,"outputs":[{"output_type":"execute_result","data":{"text/plain":["CaptionNet(\n","  (cnn_to_h0): Linear(in_features=512, out_features=256, bias=True)\n","  (cnn_to_c0): Linear(in_features=512, out_features=256, bias=True)\n","  (emb): Embedding(10403, 128, padding_idx=3, max_norm=2)\n","  (attention): Attention(\n","    (scorer): ScaledDotProductScore()\n","  )\n","  (cnn_to_attn_key): Linear(in_features=512, out_features=256, bias=True)\n","  (lstm): LSTMCell(640, 256)\n","  (logits_mlp): Sequential(\n","    (0): Linear(in_features=896, out_features=128, bias=True)\n","    (1): ReLU()\n","    (2): Dropout(p=0.001, inplace=False)\n","    (3): Linear(in_features=128, out_features=10403, bias=True)\n","    (4): ReLU()\n","  )\n",")"]},"metadata":{"tags":[]},"execution_count":422}]},{"cell_type":"markdown","metadata":{"id":"18-BClv4u8y3"},"source":["### Apply trained model\n","\n","Let's unpack our pre-trained VGG network and see what our model is capable of."]},{"cell_type":"code","metadata":{"id":"XkXEZaEYu8y3","executionInfo":{"status":"aborted","timestamp":1606226417301,"user_tz":-180,"elapsed":451098,"user":{"displayName":"Максим Евгеньевич Христолюбов","photoUrl":"","userId":"07722095761291801978"}}},"source":["from torch import nn\n","import torch.nn.functional as F\n","from torchvision.models.vgg import VGG, cfgs as VGG_cfgs, make_layers\n","from warnings import warn\n","class BeheadedVGG19(VGG):\n","    \"\"\" Like torchvision.models.inception.Inception3 but the head goes separately \"\"\"\n","    \n","    def forward(self, x):\n","        x_for_attn = x= self.features(x)\n","        x = self.avgpool(x)\n","        x = torch.flatten(x, 1)\n","        logits = x = self.classifier(x)\n","        return x_for_attn, logits\n","    \n","features_net = BeheadedVGG19(make_layers(VGG_cfgs['E'], batch_norm=False), init_weights=False)\n","\n","from torch.utils.model_zoo import load_url\n","features_net_url = 'https://download.pytorch.org/models/vgg19-dcbb9e9d.pth'\n","features_net.load_state_dict(load_url(features_net_url))\n","\n","features_net = features_net.train(False)\n","if torch.cuda.is_available():\n","    features_net = features_net.cuda()\n","features_net = nn.DataParallel(features_net)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4NWOOFTmu8y3"},"source":["### Generate caption\n","\n","The function below creates captions by sampling from probabilities defined by the net.\n","\n","The implementation used here is simple but inefficient (quadratic in lstm steps). We keep it that way since it isn't a performance bottleneck."]},{"cell_type":"code","metadata":{"id":"AARD3RNKu8y3","executionInfo":{"status":"aborted","timestamp":1606226417302,"user_tz":-180,"elapsed":451097,"user":{"displayName":"Максим Евгеньевич Христолюбов","photoUrl":"","userId":"07722095761291801978"}}},"source":["def generate_caption(image, caption_prefix = (\"#START#\",), \n","                     t=1, sample=True, max_len=100):\n","    \n","    assert isinstance(image, np.ndarray) and np.max(image) <= 1\\\n","           and np.min(image) >=0 and image.shape[-1] == 3\n","    \n","    image = torch.tensor(image.transpose([2, 0, 1]), dtype=torch.float32)\n","    \n","    vectors_9x9, logits = features_net(image[None])\n","    caption_prefix = list(caption_prefix)\n","    \n","    attention_maps = []\n","    \n","    for _ in range(max_len):\n","        \n","        prefix_ix = as_matrix([caption_prefix])\n","        prefix_ix = torch.tensor(prefix_ix, dtype=torch.int64)\n","        input_features = vectors_9x9.view(vectors_9x9.shape[0], vectors_9x9.shape[1], -1)\n","        if next(network.parameters()).is_cuda:\n","            input_features, prefix_ix = input_features.cuda(), prefix_ix.cuda()\n","        else:\n","            input_features, prefix_ix = input_features.cpu(), prefix_ix.cpu()\n","        next_word_logits, cur_attention_map = network(input_features, prefix_ix)\n","        next_word_logits = next_word_logits[0, -1]\n","        cur_attention_map = cur_attention_map[0, -1]\n","        next_word_probs = F.softmax(next_word_logits, -1).detach().cpu().numpy()\n","        attention_maps.append(cur_attention_map.detach().cpu())\n","        \n","        assert len(next_word_probs.shape) ==1, 'probs must be one-dimensional'\n","        next_word_probs = next_word_probs ** t / np.sum(next_word_probs ** t) # apply temperature\n","\n","        if sample:\n","            next_word = np.random.choice(vocab, p=next_word_probs) \n","        else:\n","            next_word = vocab[np.argmax(next_word_probs)]\n","\n","        caption_prefix.append(next_word)\n","\n","        if next_word==\"#END#\":\n","            break\n","\n","    return caption_prefix, attention_maps"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"p1VfqtzIu8y3"},"source":["Here is the code which downloads image, prints different generated captions and visualize attention map."]},{"cell_type":"code","metadata":{"id":"QCgHzzXXu8y3","executionInfo":{"status":"aborted","timestamp":1606226417303,"user_tz":-180,"elapsed":451094,"user":{"displayName":"Максим Евгеньевич Христолюбов","photoUrl":"","userId":"07722095761291801978"}}},"source":["from matplotlib import pyplot as plt\n","from skimage.transform import resize\n","from tempfile import mktemp\n","from os import remove\n","%matplotlib inline\n","\n","# get and preprocess image\n","def obtain_image(filename=None, url=None):\n","    if (filename is None and url is None) or (filename is not None and url is not None):\n","        raise ValueError('You shoud specify either filename or url')\n","    if url is not None:\n","        tmpfilename = mktemp()\n","        !wget {url} -O {tmpfilename} -q\n","        img = plt.imread(tmpfilename)\n","        remove(tmpfilename)\n","    else:\n","        img = plt.imread(filename)\n","    img = resize(img, (299, 299), mode='wrap', anti_aliasing=True).astype('float32')\n","    return img\n","\n","def show_img(img):\n","    plt.imshow(img)\n","    plt.axis('off')\n","\n","def print_possible_captions(img, num_captions=10, temperature=5.):\n","    for i in range(num_captions):\n","        print(' '.join(generate_caption(img, t=temperature)[0][1:-1]))\n","        \n","def draw_attention_map(img, caption, attention_map):\n","    s = 4\n","    n = len(caption)\n","    w = 4\n","    h = n // w + 1\n","    plt.figure(figsize=(w * s, h * s))\n","    plt.subplot(h, w, 1)\n","    plt.imshow(img)\n","    plt.title('INPUT', fontsize=s * 4)\n","    plt.axis('off')\n","    for i, word, attention in zip(range(n), caption, attention_map):\n","        plt.subplot(h, w, 2 + i)\n","        attn_map = attention.view(1, 1, 9, 9)\n","        attn_map = F.interpolate(attn_map, size=(12, 12), mode='nearest')\n","        attn_map = F.interpolate(attn_map, size=(299, 299), mode='bilinear', align_corners=False)\n","        attn_map = attn_map[0, 0][:, :, None]\n","        attn_map = torch.min(attn_map / attn_map.max(), torch.ones_like(attn_map)).numpy()\n","        plt.imshow(img * attn_map)\n","        plt.title(word, fontsize=s * 4)\n","        plt.axis('off')\n","\n","def process_image(img):\n","    print_possible_captions(img)\n","    c, am = generate_caption(img, t=5.)\n","    draw_attention_map(img, c[1:-1], am)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"EEVbfSmku8y3","executionInfo":{"status":"aborted","timestamp":1606226417303,"user_tz":-180,"elapsed":451092,"user":{"displayName":"Максим Евгеньевич Христолюбов","photoUrl":"","userId":"07722095761291801978"}}},"source":["img = obtain_image(url=\"https://thecenterformindfuleating.org/resources/Pictures/asian%20man%20eating.jpg\")\n","show_img(img)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0G1lPDipu8y3","executionInfo":{"status":"aborted","timestamp":1606226417305,"user_tz":-180,"elapsed":451091,"user":{"displayName":"Максим Евгеньевич Христолюбов","photoUrl":"","userId":"07722095761291801978"}}},"source":["process_image(img)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mb_I2Ycqu8y3","executionInfo":{"status":"aborted","timestamp":1606226417305,"user_tz":-180,"elapsed":451090,"user":{"displayName":"Максим Евгеньевич Христолюбов","photoUrl":"","userId":"07722095761291801978"}}},"source":["process_image(obtain_image(url=\"https://www.dimensioneattiva.it/wp-content/uploads/2016/10/Bike5.jpg\"))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xpaMiGYLu8y3","executionInfo":{"status":"aborted","timestamp":1606226417306,"user_tz":-180,"elapsed":451089,"user":{"displayName":"Максим Евгеньевич Христолюбов","photoUrl":"","userId":"07722095761291801978"}}},"source":["process_image(obtain_image(url=\"http://ccanimalclinic.com/wp-content/uploads/2017/07/Cat-and-dog-1.jpg\"))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"q5BrizA5u8y3","executionInfo":{"status":"aborted","timestamp":1606226417306,"user_tz":-180,"elapsed":451088,"user":{"displayName":"Максим Евгеньевич Христолюбов","photoUrl":"","userId":"07722095761291801978"}}},"source":["process_image(obtain_image(url=\"https://pixel.nymag.com/imgs/daily/selectall/2018/02/12/12-tony-hawk.w710.h473.jpg\"))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mZOwCzYfu8y3"},"source":["# Demo\n","### Find at least 10 images to test it on.\n","* Seriously, that's part of an assignment. Go get at least 10 pictures to get captioned\n","* Make sure it works okay on __simple__ images before going to something more comples\n","* Photos, not animation/3d/drawings, unless you want to train CNN network on anime\n","* Mind the aspect ratio"]},{"cell_type":"code","metadata":{"id":"00b1hRcLu8y3","executionInfo":{"status":"aborted","timestamp":1606226417307,"user_tz":-180,"elapsed":451087,"user":{"displayName":"Максим Евгеньевич Христолюбов","photoUrl":"","userId":"07722095761291801978"}}},"source":["process_image(obtain_image(filename = attention_path + 'MyPhoto/att1.jpg'))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0mV_7318u8y3","executionInfo":{"status":"aborted","timestamp":1606226417307,"user_tz":-180,"elapsed":451086,"user":{"displayName":"Максим Евгеньевич Христолюбов","photoUrl":"","userId":"07722095761291801978"}}},"source":["process_image(obtain_image(filename = attention_path + 'MyPhoto/att2.jpg'))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5lXqrVw4kkUH","executionInfo":{"status":"aborted","timestamp":1606226417307,"user_tz":-180,"elapsed":451084,"user":{"displayName":"Максим Евгеньевич Христолюбов","photoUrl":"","userId":"07722095761291801978"}}},"source":["process_image(obtain_image(filename = attention_path + 'MyPhoto/att3.jpg'))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LOVPVYHnkkaw","executionInfo":{"status":"aborted","timestamp":1606226417308,"user_tz":-180,"elapsed":451083,"user":{"displayName":"Максим Евгеньевич Христолюбов","photoUrl":"","userId":"07722095761291801978"}}},"source":["process_image(obtain_image(filename = attention_path + 'MyPhoto/att4.jpg'))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"B3WzGES_kkdd","executionInfo":{"status":"aborted","timestamp":1606226417308,"user_tz":-180,"elapsed":451082,"user":{"displayName":"Максим Евгеньевич Христолюбов","photoUrl":"","userId":"07722095761291801978"}}},"source":["process_image(obtain_image(filename = attention_path + 'MyPhoto/att5.jpg'))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LTNpjODLkkgM","executionInfo":{"status":"aborted","timestamp":1606226417309,"user_tz":-180,"elapsed":451081,"user":{"displayName":"Максим Евгеньевич Христолюбов","photoUrl":"","userId":"07722095761291801978"}}},"source":["process_image(obtain_image(filename = attention_path + 'MyPhoto/att6.jpg'))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RUR36tOtkkiX","executionInfo":{"status":"aborted","timestamp":1606226417309,"user_tz":-180,"elapsed":451080,"user":{"displayName":"Максим Евгеньевич Христолюбов","photoUrl":"","userId":"07722095761291801978"}}},"source":["process_image(obtain_image(filename = attention_path + 'MyPhoto/att7.jpg'))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vxY8GJEWkklE","executionInfo":{"status":"aborted","timestamp":1606226417309,"user_tz":-180,"elapsed":451078,"user":{"displayName":"Максим Евгеньевич Христолюбов","photoUrl":"","userId":"07722095761291801978"}}},"source":["process_image(obtain_image(filename = attention_path + 'MyPhoto/att8.jpg'))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"s1il8ecakknk","executionInfo":{"status":"aborted","timestamp":1606226417310,"user_tz":-180,"elapsed":451078,"user":{"displayName":"Максим Евгеньевич Христолюбов","photoUrl":"","userId":"07722095761291801978"}}},"source":["process_image(obtain_image(filename = attention_path + 'MyPhoto/att9.jpg'))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"sHGHL8eckkqh","executionInfo":{"status":"aborted","timestamp":1606226417310,"user_tz":-180,"elapsed":451076,"user":{"displayName":"Максим Евгеньевич Христолюбов","photoUrl":"","userId":"07722095761291801978"}}},"source":["process_image(obtain_image(filename = attention_path + 'MyPhoto/att10.jpg'))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NxgZm7KHkktN","executionInfo":{"status":"aborted","timestamp":1606226417311,"user_tz":-180,"elapsed":451075,"user":{"displayName":"Максим Евгеньевич Христолюбов","photoUrl":"","userId":"07722095761291801978"}}},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xOmn8MYhu8y3"},"source":["### Conclusion\n","Here is a place for your conclusions, observations, hypotheses, and any other feedback."]},{"cell_type":"markdown","metadata":{"id":"CihY7GTAu8y3"},"source":["### Now what?\n","\n","Your model produces some captions but you still strive to improve it? You're damn right to do so. Here are some ideas that go beyond simply \"stacking more layers\". The options are listed easiest to hardest.\n","\n","##### Attention\n","You can build better and more interpretable captioning model with attention over the generated part of the sentense.\n","\n","##### Subword level captioning\n","In the base version, we replace all rare words with UNKs which throws away a lot of information and reduces quality. A better way to deal with vocabulary size problem would be to use Byte-Pair Encoding\n","\n","* BPE implementation you can use: [github_repo](https://github.com/rsennrich/subword-nmt). \n","* Theory: https://arxiv.org/abs/1508.07909\n","* It was originally built for machine translation, but it should work with captioning just as well.\n","\n","#### Reinforcement learning\n","* After your model has been pre-trained in a teacher forced way, you can tune for captioning-speific models like CIDEr.\n","* Tutorial on RL for sequence models: [practical_rl week8](https://github.com/yandexdataschool/Practical_RL/tree/master/week8_scst)\n","* Theory: https://arxiv.org/abs/1612.00563"]}]}